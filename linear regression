import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the Boston Housing dataset
boston = load_boston()

# Convert the dataset to a Pandas DataFrame
boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)
boston_df['MEDV'] = boston.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    boston_df[boston.feature_names], boston_df['MEDV'], test_size=0.2, random_state=42)

# Create and fit the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
print(f'MSE: {mse:.2f}, RMSE: {rmse:.2f}, R^2: {r2:.2f}')

# Create a function to perform k-fold cross-validation
def cross_val(X, y, k=10):
    fold_size = X.shape[0] // k
    scores = []
    for i in range(k):
        val_start = i * fold_size
        val_end = (i+1) * fold_size
        X_val = X.iloc[val_start:val_end, :]
        y_val = y.iloc[val_start:val_end]
        X_train = pd.concat([X.iloc[:val_start, :], X.iloc[val_end:, :]])
        y_train = pd.concat([y.iloc[:val_start], y.iloc[val_end:]])
        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        scores.append(mean_squared_error(y_val, y_pred))
    return np.mean(scores)

# Evaluate the model using k-fold cross-validation
k = 10
cv_score = cross_val(boston_df[boston.feature_names], boston_df['MEDV'], k=k)
print(f'{k}-fold Cross-validation score: {cv_score:.2f}')

# Create a function to generate polynomial features
def generate_poly_features(X, degree=2):
    poly = np.zeros((X.shape[0], X.shape[1]*degree))
    for i in range(X.shape[1]):
        for j in range(1, degree+1):
            poly[:, i*degree+j-1] = np.power(X[:, i], j)
    return poly

# Generate polynomial features
X_train_poly = generate_poly_features(X_train.values, degree=2)
X_test_poly = generate_poly_features(X_test.values, degree=2)

# Create and fit the polynomial regression model
model_poly = LinearRegression()
model_poly.fit(X_train_poly, y_train)

# Predict on the test data
y_pred_poly = model_poly.predict(X_test_poly)

# Evaluate the polynomial model
mse_poly = mean_squared_error(y_test, y_pred_poly)
rmse_poly = np.sqrt(mse_poly)
r2_poly = r2_score(y_test, y_pred_poly)
print(f'Polynomial Regression - MSE: {mse_poly:.2f}, RMSE: {rmse_poly:.2f}, R^2: {r2_poly:.2f}')
